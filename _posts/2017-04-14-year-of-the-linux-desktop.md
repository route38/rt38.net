---
title: Why it will never be the year of the Linux desktop
layout: post
image: "/art/heroes/desk.jpg"
description: "Linux has been a runaway success, but it'll never succeed on the desktop.  I've been at this for 11 years.  Ain't gonna happen."
mood: definitive
music: "Deep Blue Something &mdash; Breakfast at Tiffany's"
coffee: "Iced"
category: Linux
---

Let it be known before I begin that I love Linux.  I think it's a splendid
operating system with truckloads of potential and only a few major setbacks.  It
is fantastic if you need an industrial-strength server operating system, or
something you can pare down to the blood and bones for an embedded system, or
something you can hack on if you just want a system to learn on.  But student
hackers, embedded system programmers, and server admins aren't the marketplace
penguinistas think Linux needs to reach.  We make this proclamation every year,
blissfully unaware of how ridiculous it is, and indeed moreso becomes, with each
passing year.  2017 is _so totes_ the year of the Linux desktop, you guys!!1!

## The Year of the Unix desktop

2001 was the year of the Unix desktop, for that was the year that Apple
introduced the successor to Mac OS 9, that being the long-lived product Mac OS
X.  Indeed, although it took Apple a couple of releases to hammer out all the
bugs and make the experience pleasant enough for the average Mac user, Mac OS X
succeeded at drawing Unix nerds, not just Mac users, into the fold.  A 2002 ad
rather boastfully proclaimed, using the still-sleek Titanium PowerBook G4, that
the Mac sent all other Unix boxes to `/dev/null`.  Apple had a confluence of
four factors that led into making Mac OS X a success:

1. Hardware advantages over the Pentium IV processors of the day, particularly
   when the Power Mac G5 entered the scene in 2003.  Apple still offered potent
   integration between hardware and software when they switched to Intel
   processors in 2006.

2. Backward compatibility, via the _Classic_ environment, with Mac OS 9
   programs, and with much Mac OS 9 source code via the _Carbon_ API.  This
   advantage disappeared with the Intel transition, but by then it was largely
   unneeded.  Carbon has now been deprecated for years as well.
   
3. A developer-friendly API, if one using an offbeat language, with Cocoa and
   Objective-C.  Apple now promotes the _Swift_ language, and with some changes,
   the same API works for programming on both iOS and macOS (as well as watchOS
   and tvOS, Apple's other OS products).

4. Aggressive marketing and alluring products that looked nothing like PCs of
   the period.  Indeed, Apple's aggressive marketing, plus the halo effect of
   products like iPhone, iPad, and iPod, got Macs into the hands of a lot of
   people -- your humble host included.  (I got an iPod nano in 2006, and it
   only served to convince me even harder to buy a Mac.)

Apple did, in 2001, what no Linux integrator or community project could: it
deployed a Unix-like operating system to the general public that offered the
same legendary ease-of-use and commercial application support as Microsoft
Windows or their own Mac OS 9, thus bringing the reliability and openness of
Unix to the masses.  Outside of the server market, the Linux community has never
succeeded at doing this.

In fairness, Apple had at least five things in their favor as far as OS X was
concerned:

1. The usability experts, or at least the results of their work, from the
   original Mac OS.

2. The IP rights to the Macintosh interface.

3. A relentless perfectionist in Steve Jobs, peerless industrial
   design with Jony Ive, and excellent systems development and leadership
   on the part of Avie Tevanian.

4. The NextStep codebase, including the Cocoa API.

5. The aforesaid marketing campaigns and alluring products, along with strong
   product integration.  You could buy an iPod and a PowerBook and be guaranteed
   that the one could download music from the other, for instance, a guarantee
   you didn't necessarily have on other platforms with competing products.

And it's true that in recent years, with discontentment continuing to
foment within the Windows community, that many people have switched to
the Mac.  (It also helps that the halo effect has become even more
substantial than it used to be, and that Apple has repositioned itself
as a luxury brand, such that ownership of its computers is considered a
status symbol.  Your overpriced Intel commodity hardware stuffed into
impractical, difficult-to-service cases and supported by an overly
litigious company clinging desperately to relevance in the desktop space
doesn't impress me, but _eh_ -- to each her own.)

## A story of stratification

To be fair, Linux’s market share is less marginal than it used to be.
Reviewers look at modern desktop environments and they’re generally
satisfied with what they see, even if for reasons I find inexplicable.
Like Microsoft (vis-à-vis Windows 8, for example), and Apple (vis-à-vis
Mac OS X Lion), the GNOME and KDE projects, as well as Ubuntu’s
execrable Unity effort, have gratuitously changed the desktop with no
perceptible net benefit.

### Microsoft's missteps: Windows 10 and 8

I admitted years ago that Windows 8 did make a few token improvements to
the Windows formula, including bringing productivity to tablet PCs.
Microsoft's dispensing with certain parts of the Windows experience,
like the baffling omission of the Start menu, still bugs me years later.
Microsoft, in developing Windows 95, hired UI/UX experts and extensively
tested what became the classic Windows interface.  For 17 years, each
Windows release was an iterative improvement on this interface, even if
some of the improvements were questionable at best (like creating a
whole new class of security holes by embedding a web browser in a file
manager!).  On the whole, though, even I found Windows 7 reasonably
pleasant to use, which from my position is really saying something.  The
fact that recent versions of Windows spy on you, and are about as
trustworthy as Boris and Natasha put together, is what drove me away
from the platform outright, outside of a few small tasks that are just
not as easy to do in other systems, like gaming.

Windows 10 is what happens when you put Windows 8 on steroids: the good
parts are better, the bad parts are worse, and it gets even crazier.
The notion of turning Windows, the OS, into a subscription product like
a magazine or an ISP's service contract does three things to me:

1. It scares the crap out of me.  Client operating systems are not
   conducive to the subscription model, especially in the enterprise,
   nor is the idea of turning Windows into a semi-rolling-release kind
   of project à la Arch Linux.

2. It makes me laugh my head off at Microsoft for thinking this is even
   a remotely good idea.  Well, Red Hat may think it's a good idea.  If
   Apple had any desire to make inroads into the enterprise, it might
   also be a viable time for them!

3. It makes me wonder what kind of exquisitely delusion-forming crack
   Satya Nadella and company are smoking.

### Macintosh and change for change's sake

2011 was Apple's _annum horriblis_.  Yes, 1997 was bad, but at least
there was NextStep available to buy out.  It's much harder to reanimate
the dead, and Steve Jobs's death capped a terrible year for the Mac OS,
and for the Mac platform generally.  That was, you see, the year that OS
X Lion came out, and Lion altered the Mac platform in ways it never
recovered from.  The reworking of Exposé was appreciated at the time, as
was the fundamental reconsideration of the Dashboard, but the iOS
scrollbars were distracting (and something that the Linux community
eagerly copied wholesale!), and the ability to resize windows from all
sides, while nice, is perplexing.  Why change after 27 years, I wondered
at the time.  Similarly, I didn’t grok the importance, significance, or
utility of full-screen applications (aside from viewing video) on a
traditional desktop or laptop.  Some writers swear by their full-screen
editors; the few times I tried using them, I swore _at_ them.
Full-screen web browsing is for phones and tablets, not laptops or
desktops.  Full-screen ebook reading is for phones and tablets, not
laptops or desktops.  Full-screen photo editing is for tablets, not
desktops.  (You get the gist.)

Building a versioning file system into macOS, I must confess, is a
stroke of genius.  It saves versions of files, rather like Git or
Mercurial, which is rather nice since you can revisit older versions if
you make a mistake or just want to visit your earlier drafts for some
reason.  But if you're like I was toward the end of my MacBook's life,
and you only seldom use your Mac, then you discover the dark side to
this functionality.  After a predetermined interval (I think it's six
months?) macOS locks your files for further editing until you unlock
them.  This is a pain in the backside for two reasons: first, like I
said, I seldom used my Mac. Second, I have thousands of files scattered
across more storage media than I care to cop to.

If I find a file that I want to change, but I last modified it years
ago, that should be my right, my prerogative as a computer user.  I know
what I'm doing, but Apple doesn't know that, and there is -- as far as I
know -- no means of disabling this behavior.  I can _kinda_ see the
potential utility here, as it stops clueless users from doing
clueless-user-kinda-things, but I am not, nor have I ever been, a
completely clueless user.  I resent the implication on Apple's part that
I'm too dumb to know what I'm doing.  I'm not, and I don't need
protection from myself.  The few times when I do need protection from
myself generally involve sleep deprivation or inebriation, both of them
situations when I don't need to be administrating a system or looking at
love letters I wrote in middle school.

### When Linux raises you a certain finger

GNOME 2 was a handsomely traditional user interface and one I loved
greatly.  It was just customisable enough, with plenty of information at
my fingertips, and it ran efficiently on any computer I dared to run
Linux on, from an ageing Dell Latitude from 2000 to a high-end gaming PC
built in 2009.  It was, and -- as MATE -- remains, a brilliant desktop
environment.

GNOME 3, on the other hand, in its first releases anyway, made me feel
as though my fingers had been hacked off by a reciprocating saw.  "Ease
of use" is subjective, which I will freely admit, but GNOME 3 was not
easy for me to use.  It was difficult to get accustomed to an
application screen, as opposed to the traditional menu.  It was
difficult to find the settings that allowed me to have an environment
more like GNOME 2.  It was difficult to understand why, when I installed
GNOME 3 (via Fedora) on my Chromebook Pixel, that the hi-DPI settings
were inconsistent at best.  It remains difficult, years later, to
understand why the GNOME team seems insistent upon removing, rather than
adding, functionality to the various parts of the desktop.

KDE, in its various iterations, remains the champion of the lot next to
MATE.  KDE 4 was awesome in 2013, and KDE 5 is awesome now, both of
these desktops eminently pleasant to work in, neither one of them
shitting openly on 20+ years of UI/UX research on the part of both
private and public organizations.  I find the Oxygen (KDE 4) and Breathe
(KDE 5) appearances quite neater than GNOME 3, while Dolphin is the file
browser that makes the others shrivel up like a cold day in the locker
room.  Kicker is an excellent panel, and the system settings are just as
encyclopedic as ever, letting me hone in on exactly the settings that I
want, for exactly the environment that I need.

The less I say about Unity, the better.  It rips off the Macintosh in
every way that matters, and has absolutely no merit whatsoever in that
regard.  It was crap in a box in 2013 and it's crap in a box now.  At
least I can download an image that lets me use something that isn't
Unity if I insist on having Ubuntu.  XFCE is still decent, but I haven't
tried Cinnamon, and I really see no point in it.

### The Disadvantage of inconsistency

After viewing the landscape, why is Linux so disadvantaged?

At first blush, the environments aren’t so dissimilar: they all have the same
fundamental parts — windows, icons, menus, and a pointer.  The similarities end
there, though, because macOS and Windows both have something Linux never will:
*consistency*.

This is consistency: If I sit down at my iBook, with its 87-key keyboard and
single-button trackpad, I can find my way around.  I know where stuff is; if I
need to write a love letter, for example, I know where Pages is, whereas if I
need to hop on the Internet, Safari is just as easy to find, and is in the exact
same place (the system-wide `/Applications` directory).  If I somehow found
myself in front of John Gruber's Mac Pro, with its full keyboard and
conventional mouse, and a far newer version of macOS, I could still find my way
around.

This too is consistency: If I sit down at my adolescent Compaq, with its
full keyboard and Windows XP installation, I can find Microsoft Word or
Google Chrome for my love-letter or web-browsing needs.  If I somehow
found myself at Microsoft HQ, sitting at Satya Nadella's Surface Book
(because you know the dude's running one), with Windows 10 and its
constrained keyboard, I'd be just as able to find Word and a browser
(probably Edge) to do the same things I'd do on the old Compaq.

A Linux machine affords you no such consistency.  Between the three
Linux desktop environments I use on a regular basis, I can count on a
similar amount of applications (e.g., Chromium, LibreOffice, Gimp,
etc.), but the similarities end there.  In Slackware, for instance, I'm
using KDE 4, whereas I'm using the latest and greatest version of MATE
on my Arch installation.  And on Kubuntu, my work OS, I'm running Plasma
5, which is similar enough to KDE 4 that I know what I'm doing -- but
it's still different.  The only guaranteed way to know for sure that I'm
doing the same thing, the same way, is to standardize on one version of
one distro for my desktop needs.  (On my servers, I don't even have a
desktop.  They don't count.)

While there are a few commonalities, the amount of choice is stupefying: there
are at least three browsers to choose from, two office suites, five command
shells, six desktop environments, and hundreds of distributions that package
that software in varying ways, with several different styles of package
manager.  Just looking at three parts of the experience show the differences at
play:

* Fedora
  * Package manager: `rpm`/`dnf`
  * Desktop: GNOME 3
  * Init: systemd
* Gentoo
  * Package manager: `portage`/`emerge`
  * Desktop: User's choice
  * Init: OpenRC (systemd available if you want it)
* Slackware
  * Package manager: None (but we have SlackBuilds and manipulation tools)
  * Desktop: Pat's choice (KDE)
  * Init: sysvinit
* Linux Mint (an Ubuntu derivative)
  * Package manager: `dpkg`/`apt`
  * Desktop: Cinnamon
  * Init: systemd
* Arch Linux
  * Package manager: `pacman`
  * Desktop: User's choice
  * Init: systemd

While I can set up all five of these distros with the same applications, the
means of doing so is different, and in the case of Gentoo, involves several days
of compilation from source code.  My usual userland consists of:

* Shell: `zsh`
* Editor: `vim`
* Word processor: LibreOffice Writer
* Spreadsheet: LibreOffice Calc
* Browser: Chrome or Chromium
* Mailer: Claws-Mail
* Graphics: Gimp, Inkscape, and Darktable
* File manager: Caja or Dolphin
* Terminal emulator: MATE-Terminal or Konsole
* Audio editor: Audacity

Anything not listed here is something that I don't do on the desktop
(i.e., I do it in the cloud, as with music and the bulk of my document
editing).  Several of these programs, except for Gimp, Inkscape,
Audacity, and Darktable, have several other programs of similar scope to
compete against.

There's no blanket set of instructions you can give a Linux user and simply say
to them, "Do it this way," as you can with Windows or macOS.  That makes
supporting a large amount of Linux users, in a heterogeneous environment, a
nightmare, assuming it's even doable.  As I said above, your only true recourse
is to standardize on a single desktop distribution, with a uniform update
schedule, such that you can offer blanket support across the organization on
your internal wiki or wherever.  That restricts you to a handful of distros
right off the bat unless you prepare a standard image of some hackerish distro
like Arch or Gentoo.  If your organization requires a predictable release cycle,
you're further restricted -- Debian and Slackware follow the "when it's ready"
release schedule, while Ubuntu has new releases every six months (April and
October) with long-term-support releases coming in April of even-numbered
years.  (E.g., Hardy Heron, 8.04, was LTS, while its predecessor, Gutsy Gibbon,
7.10, wasn't, nor its successor, Intrepid Ibex, 8.10.)  Fedora has a similar
release schedule, but doesn't have long-term support releases.  (They really
want you buying Red Hat licenses if you need long-term support.  Or you could
just use CentOS if you don't need paid support.)

I think the craving for consistency is the major reason why Ubuntu has taken off
with such force in the Linux community.  It's a consistent OS, even if the
latest version of Unity is still a hot mess, and an impressive amount of
infrastructure has risen around Ubuntu.  That infrastructure has improved the
Debian community, as Ubuntu is based on Debian and contributes patches back to
it.  I've been a Debian user for years -- almost 11 years, in fact.  It's a good
OS, one that I still like using as a server OS where I want two things and only
two things: predictability and maintainability.  Ubuntu is practically
idiot-proof, and Linux Mint goes one step further.

## Consistency in your pocket; or, how Google and Apple both get it right

The need for consistency is even more pronounced with that little computer you
carry around in your pocket.  Whether you're an Android user or an Apple fan,
you expect your phone to behave a certain way, and when you upgrade to the
latest and greatest, you expect to continue using it much the same way, whether
you acknowledge it or not.  Love it or hate it, iOS and well-written apps for it
behave in easily predicted ways.  The same is mostly true of Android and
well-made Android apps.

Perhaps the craving for consistency is also why Google's Chrome OS has made
inroads in markets where consistency reigns supreme.  Chromebooks are hugely
successful in the education market, where a cheap, easily replaced laptop is a
major blessing in the face of abusive, and otherwise clueless, children.  In my
own case, I admire the ease of use of Chrome OS and the power of the cloud,
along with the total lack of maintenance on my part.  I only need to reboot my
Chromebook Pixel occasionally to install new updates.  That's it.  I don't have
to fight with the command line.  I don't have to futz around with conflicting
patches to packages I'm barely even aware exist.  I don't, in the case of
Windows or macOS, wonder what part of my computing freedom the vendor is taking
away from me now, or what oldie-but-goodie program is going to stop working with
this release.  I don't even have to _remember_ to check for updates -- Chrome OS
does it automatically, letting me continue to focus on my work.  It has the
advantages of a Macintosh in that regard -- it just lets me do my thing, it
doesn't take forever to update, and the interface is at once predictable, easy
to use, and beautiful to behold.  Chrome OS has the added benefit that 99% of
what I do on it is in the cloud, and the last 1% is unimportant if I should lose
my Chromebook, or if it should be stolen.  If a thief makes off with my
Slackware-powered IdeaPad, for example, I lose a _lot_ of working state whose
backup status is questionable.  I have no such concern with my Chromebook, aside
from losing a laptop I spent $400 on.

## The Linux power vacuum

Overall, the Linux community seems to embrace anarchy.  If you don't like
something, you just fork it and continue.  The bewildering array of distros,
init systems, shells, desktop environments, applications, package managers,
servers (for all sorts of tasks!), and other programs is a testament to the
politicized nature of the free software movement.  Some Debianites, incensed at
that distro's move to systemd for version 8, forked the distro into what we
call _Devuan_, or Debian without systemd.  Some Arch users, frustrated at how
"difficult" Arch is to install, forked the project into Manjaro and Antergos,
two of the growing set of Arch-based distros.  Some Gentoo users, frustrated at
their distro of choice, forked it into Sabayon.  The kernel itself has several
divergent versions, offering different schedulers for desktop and studio use.
There's the well-known schism between the Apache Foundation's OpenOffice and the
Document Foundation's LibreOffice, or the much older schism between GNU Emacs
and `jwz`'s XEmacs, both parts of the enduring holy war between Emacs users and
Vim users.  For years, Debian refused to use the insignia and name of Firefox in
its distribution, so it took the latest Firefox, stripped it of any Mozilla
names, and called it Iceweasel.  (They did the same thing to Thunderbird/Icedove
and SeaMonkey/Iceape.)

The impressive _lack_ of splintering in certain system components shows how deep
they run into the system, or how great the cost would be to write them anew.
The Linux kernel, despite various schedulers for various uses, endures.  So too
do the GCC compilers (and the rest of the GNU build system, for that matter,
even in the face of CMake and newer tools for other languages), the GIMP image
editor, the major browser rendering engines, X11, and the major widget
toolkits.  I realize that Wayland is making significant inroads into X11's
foothold, but it will be at least a decade before X11 dies off completely, if it
ever does.

But here's the thing.  Leadership in the Linux community isn't really about
merit; it's about lack of viable alternatives.  If LibreOffice were really
_that_ good, Microsoft would consider it a true, viable threat to their Office
products.  (They don't.  In fact, they're more concerned with Google Docs
eroding their market share.)  If The Gimp, Darktable, or Inkscape were really
that good, Adobe would be quaking in their boots.  (They're not.  Photoshop,
Lightroom, and Illustrator have unshakable grips on their respective markets.)
For every Windows or Mac user who says, "I'd switch to Linux, but it doesn't run
{Program X}," there are ten more to whom the idea of switching never occurs, and
if it does, it's from Windows to the Mac, not from Windows or the Mac to Linux.

I love Linux as a server OS.  But I began using it on the desktop, and
I've been a desktop Linux user for long enough to be able to say that I
prefer it to Windows or macOS.  I wouldn't use anything else (except,
maybe, BSD or illumos) on the server, and I seriously doubt I'd use
anything else (except, possibly, BSD or Haiku) on the desktop.  If you
could view a typical Linux distribution as an integrated product (pick,
say, Debian or RHEL), then if it were really good enough, _as an
integrated product_, it would succeed in a truly free market on its own
merits.  That the only significant uptick in desktop Linux use has come
by way of Chrome OS in the past ten years tells me:

1. That Linux is simply not meant to be a viable alternative to
   commercial desktop operating systems (that is to say, its niche is
   firmly on the server and in embedded systems).  (I call bullshit
   here, because Linux-on-the-desktop has been a mature enough product
   for me to use it, for at least 50% of my computing, since 2006.)
   Alternatively...

2. The market is not truly free and it is difficult, bordering on
   impossible, to compete against Microsoft and/or Apple.  (Plausible,
   but it stinks like a cop-out to me, especially in the face of
   companies like System76 and Purism (and Dell!!) that prove that
   selling Linux-powered computers is a viable business model.)

I think that Linux-as-a-product is inconsistent enough that the only way
it will ever be more than a hacker's playground is if it receives
similar amounts of developer support -- _paid developer support_ at that
-- that, say, Microsoft lavishes upon desktop Windows.  Extensive
amounts of developer support combined with aggressively sought
preinstallation agreements with IHVs is the only way that Linux will
advance beyond its reputation as an OS for the extreme nerd.  For all
the money Red Hat makes at selling Linux licenses, and for all the good
they do by contributing paid developers to the Linux kernel and the
projects surrounding it, they'll never beat Microsoft at their own game,
at least not until Microsoft implodes under their own bulk and finally
accepts that Windows 10 is not a good product.

### The Conclusion

There are some undoubtable concessions to make in closing this rather lengthy
post.

First, I will happily, nay enthusiastically, concede that Linux as a kernel and
GNU/Linux as an operating system have both had a profound impact on computing.
Full stop.  In 26 years, Linux has become a multibillion-dollar industry.  The
kernel runs on everything from HTC phones to Cisco routers and from IBM
mainframes to Dell PCs.  Hardware vendors ship Linux laptops, servers, and
supercomputers, to say nothing of traditional desktops, smartphones, netbooks,
nettops, tablets, car infotainment systems, smart watches, set-top boxes, cash
machines, printer firmwares, and much, much more.  Linux isn't just a case study
on the power of open source development, but indeed on the "maker culture" as a
whole.  Without Linux, you don't have Firefox, or WebKit, or Android, or
Raspberry Pi, or Arduino.  Without Linux, you'd still be waiting for
GNU/Hurd to get off the ground, probably, and there'd be no fully free
operating system under a copyleft license, guaranteed to be _libre_
forever.

Second, I will defend Linux as a desktop OS until my dying breaths.  None
matches the power, versatility, or hardware compatibility, and that's something
I will always and vigorously defend.  Yes, sometimes it requires me to
run binary drivers (Nvidia!), and yes, sometimes it's a pain in the ass
(Canon!), but the struggles are worth it because I personally like the
environment I work in.

Third, I will concede that as a desktop Linux user, I know it's not for
everyone.  You can take three hundred dollars to your nearest computer dealer
and walk out with a brand new laptop or even a desktop with Windows or Chrome OS
pre-loaded.  It'll be an absolute piece of shit, but it'll get you online.  And
there's little better for the office than a Windows workstation, I'm sorry to
say.  If you're a hardcore creative who works in video, print, graphics, audio,
or on the Web, it's hard to beat a Mac, except with a faster Mac, of course.
And for developers and weirdo nerds like me, well, Linux is paradise.

But, fourth, you can tweak and hack on Linux until the cows come home, and then
fiddle with it some more.  For a software developer, that's a powerful
invitation to create a hugely idiosyncratic environment.  My decade with Linux
has inalterably changed how I approach the task of using a computer, and if I
can't change it to my very specific liking, I'm less interested than I would've
been otherwise.

The Linux desktop projects of the world, however, have only succeeded at doing
two things: alienating many novices who'd otherwise find value in Linux, and
alienating many experts who just want something simple, but not _too_ simple,
that they can customize more deeply than they could Windows or macOS.

This meme has gone on long enough, for it is indeed the _age_ of the
Linux-powered device, and it has long been the age of the Linux server.  But it
will never be the year of the Linux desktop.
