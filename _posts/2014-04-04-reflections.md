---
layout: post
title: "Some Gathered Reflections on Computing"
mood: thoughtful
description: "I make some observations and some bold predictions about the future of computing."
music: "The Eagles: Visions"
image: "/art/cowboys-datacenter.jpg"
image_description: "The future is nigh, and the future is datacenters.  But that's not a bad thing.  Image credit: <a href='https://www.flickr.com/photos/davidcjones/'>David Jones</a>."
---

Apple was able to captivate a generation of computer users with a one-bit display and a thoughtfully designed user interface.  Then, years later, they were able to captivate another generation with a high-density display in millions of colors and another thoughtfully designed interface.  I think people could still be productive in System 7 in monochrome, but it wouldn't be easy.  The desktop computing paradigm hasn't changed a whole lot in the last thirty years, with all the key components still present, and I contend in this post that it'll never go away entirely.

I find the accomplishment of creating and releasing Macintosh extremely impressive on the basis that Microsoft had basically the same hardware (albeit less sophisticated regarding video), the same opportunity, and the same know-how -- and they blew it.  It's the parable of the ant and the grasshopper -- through sheer perseverance, the ant is able to overcome winter's fury, while the grasshopper comes to rely upon the ant's hard work.  Naturally, in the mid-80s, the grasshopper was Microsoft, and the ant was Apple's Macintosh skunkworks.  Thirty years ago, Macintosh debuted and with the arrival of the new computing paradigm -- for Macintosh was not merely a new computer -- came a fundamental change in how we perceived information.

## An issue of cost

But Macs were initially flawed.  It wasn't the restrictions of their operating environment (and having only 128 KB of RAM will do that to a person), but instead the number that came after "Macintosh" in the sales manual.  If you think spending $1,000 on a home computer is crazy today, even fewer people could justify spending $2,500 on a Mac in 1984, along with $500 each for an ImageWriter printer and external floppy drive.  What made the Mac pill particularly bitter to swallow was the fact that extant computers outclassed it: for productivity, you bought a $2,000 DOS box with WordPerfect or WordStar, and for gaming, you bought something cheap with a 6502 or Z80, like a TRS-80 or a Commodore.  Many of the home computers of the era, like the Atari 800, Commodore 64, Apple II, and TRS-80, were able to be used with a TV set, obviating even the need for an expensive monitor.  The Commodores and Apple IIs and DOS boxes lacked the sophisticated GUI of the Macintosh, but they had something Apple's new box didn't: an installed base of software, the support of a dealer and maintenance network, and a user community.

So, the fundamental change in how we interacted with computing power was slow in coming to the masses, even if the technology was there out in the Valley.  I doubt anyone, even Steve Jobs himself, knew what they were in for when they decided, consciously or otherwise, to change the world.  In Steve's own terms, they aimed at no less than -- and succeeded at -- putting a dent in the universe.  Windows would become mainstream by the early '90s, putting Microsoft firmly on the map and establishing the company as the preeminent manufacturer of personal computing software.  Microsoft's own largesse reached fever pitch in 1995 with the release of Windows 95.  Finally, many computer users could enjoy the same simplicity and ease of use that Mac users had known since 1984.  If you count earlier attempts, then Apple users had enjoyed similar environments since the remarkably ambitious Lisa project in 1983.  (At $9,999 per unit, not many Lisas made it out of Apple's warehouses.  Government agencies loved 'em.)

## The finest hour of the desktop

Perhaps Microsoft's finest hour came when they removed DOS from their lineup once and for all, when Windows XP was released.  During the early part of the oughties, innovation in Redmond appeared to be on the up-and-up.  Windows 2000 Professional represents one of the finest efforts in the history of computing to create a desktop environment suitable for novices as well as experts, and it not only succeeded on those fronts, but it set the groundwork for all of Microsoft's subsequent efforts -- both missteps and successes.  The gray interface is still available to you if you want it provided you run Windows 7 or previous.  (Quite why you'd run Windows 8 is a puzzler, though, unless your PC didn't come with 7 pre-installed.  I like it a lot, but it's not been very successful.)  Office 2000 similarly represents a uniquely strong combination of core functionality and extra features, and best of all this unique combination doesn't "phone home" or have any other Orwellian privacy implications.  The same is true of Windows 2000.

The clear loser in terms of security, unfortunately, is Windows 2000, which was designed to meet government standards for information security that were irrelevant five years before they were used: the security standards to which Windows 2000 was held were outdated by the time they reached the President's desk to be signed into law.  So, sadly, Windows 2000 and its superlative user experience were the target of massive outbreaks of malware that destroyed Microsoft's reputation as a purveyor of securely designed operating systems and office suites.  Frankly, that reputation was never deserved, given the MS-DOS foundations of Windows 98 and 95 and the horrible decision that was green-lighting Windows Me.

If Microsoft's finest hour came in 2000 and 2001, then Apple's came from 2005 to 2007. Mac OS X Tiger and Mac OS X Leopard represent Apple's finest attempts at creating a user-oriented operating system for the modern age. Tiger laid the groundwork of having a solid Unix foundation mated to what was finally a great user interface above.  What differentiates it from Panther is that Tiger was the OS that was pre-loaded onto Intel Macs in 2006.  OS X Tiger, as I used it between November 2006 and October 2007, was a great piece of engineering and not only a true example of what the "Macintosh Way" means, but of what a great UX is generally.  Leopard improved upon it in spades, which was in itself mightily impressive, but Leopard continued the track record for strong compatibility that Tiger exemplified.  Tiger's compatibility stretched from the first of the ROM-in-RAM G3s to the preloads on the early Intel Macs.  Leopard stretched from the middle of the G4 range (officially) to Intel Macs all the way into 2009.

Rosetta, the PowerPC compatibility layer, was significant enough an engineering feat that it featured in Apple marketing copy for the early Intel Macs.  You could run PowerPC software on your new Intel Mac, which was a convincing argument for going ahead and buying Intel.  If you relied on a Windows application or two, you could also invest in a copy of Windows and dual-boot your Mac between Windows and OS X, or more impressively run Windows in a virtual machine from within Mac OS X.

Apple's pièce de résistance, however, had to be the OS I continue to use to this very day, the first Intel-only Mac OS and the last to ship with Rosetta. Snow Leopard, version 10.6, is the very greatest operating system ever produced by Apple.  There, I said it, and I stand by it.  Snow Leopard represented the very last attempts by Apple to remain traditional.  While Lion, Mountain Lion, and Mavericks all are great OSes, they too greatly modernize the computing experience.  I once took Apple to task for their dubious Lion decisions, and it's not an article I regret writing.  If anything, I find that it grows truer over time.  My primary grievance was over the automatic saving and locking of files to prevent data loss or alteration.  Auto-save is a good idea combined with a versioning file system -- as much is something even I can concede -- but locking old files is unconscionable because it wrests control from the user and, in Lion, there was no way to disable it.  I can't speak to Mountain Lion or Mavericks, but I have to imagine Apple is equally inflexible in those systems.

The period between 2001 and 2009 seems to be, to me, the golden age of the desktop operating environment.  Ubiquity, which led to widespread computer literacy, and economies of scale, which led to reduced prices, meant computers would spread far and wide.  As the default mode of interaction, the desktop paradigm was naturally very well attuned to the needs of its users.  Apple and Google just _had_ to go and fuck it up, didn't they?

## Tending toward "mobilism"

Recent versions of the Mac OS put me in the mind of Android.  iOS would be the natural comparison, since it is from there that Apple gets features to add to OS X.  However, nothing is more inflexible than an iOS device -- you can't even use local storage that's exposed directly to the user.  In Android, you can, and the device is basically as open as you're willing to make it: you can root it and install a custom firmware image; you can install apps from unknown sources via APK sideloading or via direct download of an APK to your device; finally, and most importantly, the device is wide open for custom code and debugging.  You can't say any of those about iOS.  Through jailbreaking, you can install custom IPAs, but even that is a crapshoot since Apple is very good about fixing vulnerabilities that allow jailbreaks to occur.

Mac partisans view the Mac OS as the most highly evolved graphical environment on the face of the planet.  There *may* be something to this notion, but I don't think there's a whole lot.  Windows has been around since 1985.  Next year, the Windows environment turns 30.  In many ways, Windows is as highly evolved as Mac OS -- on the desktop.  Microsoft's putative attempt at a touch interface has largely failed on devices like laptops and tablets.  It has enjoyed modest success on phones, but even there Google and Apple do better.  As in nature, evolution is relative: compare two organisms and the phrase "highly evolved" is a misnomer: one is _highly differentiated_ with respect to the other, but both (with respect to a common ancestor) are equally "highly evolved."

## The foreseeable future

I see the future pointing us in a more appliance-oriented direction.  Computing devices that basically amount to thin clients will be the future of computing for the masses.  Smartphones and tablets are already halfway there: either one of those devices accomplishes many of an average person's computing needs, and a laptop merely fills in the gaps.  I expect that within five to seven years, it will be even more acutely so in that I expect people to start taking up with devices like Chromebooks en masse.  I feel this way because these netbooks, or Internet appliances, or whatever you wish to call them, have a lot of advantages over traditional computers for end users: they're small, lightweight, easy on power, and they don't require a great deal of administration skills.  The traditional computer will live on as an enthusiast's and professional's tool, much like it used to be in the pre-Internet era.  Face it: there were three reasons why you'd have a home computer in 1989:

0.  To play games;

1.  To transact business; or,

2.  To tinker.

Twenty-five years later, we live in multi-computer households where computers are capable of everything, it seems: communication through email and social media, entertainment through audio and video, photo manipulation, basically everything you could think of that requires technological intervention or an internet connection.  We're not running computational fluid dynamics simulations; we're playing _Angry Birds_.  We're not modeling the torsional forces on the girders of a skyscraper in hurricane-force winds; we're filing our taxes.  We're not building the Linux kernel (well, most of us aren't); we're fixing up our photos from the vacation to Colorado.  If you need to do those things, there's a computer for you to do it with: the simple home box, as a simple example, is more than capable of running Photoshop Elements and Microsoft Works -- or Pixlr and Google Drive -- and nowadays most computers can do pretty nice 3D video for gaming purposes, even the ones with integrated graphics.

## Sounds kinda nice, actually

In this age of ubiquitous broadband and increasingly dedicated devices, I foresee a day -- and one coming soon, mind you -- where you'll be able to get rid of all your home boxes and stick with what's known to work right now: mobile devices, and maybe a Chromebook or two.  If you develop, or photograph, or render, or even if you just push a lot of text, you'll throw around a MacBook Pro or a ThinkPad.  Otherwise, you won't have much use for Windows or Mac OS or even Linux on the desktop.  UNIX will power all of your devices, if they're iOS or Android, but you won't realize it unless you're a geek.  If you're a PC gamer, then you'll have your Alienware box or the one you built yourself.  If you need to do <abbr title="computational fluid dynamics">CFD</abbr> or protein folding or heavy-duty financial simulations, you'll have the box you need to do it with.  Computers will become more like cars and less like tools.

When you go buy a tool at a hardware store, you have a lot of brands and a lot of options.  Take drills for instance.  The simple battery-powered pistol-grip kind of drill a housewife would buy to hang a picture with is far separated from the hammer drill a construction worker would buy for drilling through concrete.  That hammer drill is different still from the drywall gun you'd use for one purpose only: hanging drywall.  Naturally, the drywall gun is a completely different beast to the famous Hole Hawg, which drills holes.  You can ask a Milwaukee Hole Hawg to drill a hole, and it only knows one answer: whatever you say, boss.

Buying a car is a much different matter.  Some people go to the dealer, find one with the looks they like, in the color they like, and drive home after signing the papers.  Others are a little more discerning and make sure to get the V6 version with the manual gearbox, but otherwise they find a style and color, perhaps a few token options, and they run with it.  Then there are enlightened car buyers, who do research on the Internet, find out exactly what they want, and shop aggressively to find the best deal for their money relative to what they order.  Last but not least are the true gearheads, who order exactly what they want down to the pile of the carpet on the floor board, if they bother with carpet at all, and who inevitably make sure to order the P-Zero or Mickey Thompson tires.

The "give me a car, any car, as long as it's blue" people are today's tablet and Chromebook buyers.  All they want and need are devices that enable their online lifestyles, preferably at a minimal cost of money or time.  The more-discerning people may opt for a decent laptop, perhaps a MacBook Air or a nice Lenovo model.  (A nice Dell model is an oxymoron, unless it's a Latitude.)  Those truly enlightened will inevitably buy Latitudes, ThinkPads, MacBook Pros, or perhaps even a Mac Pro if they've got the dosh for it.  Then the performance freaks are today's system builders.  They'll continue to be system builders, and on top of it they'll also continue to be the purchasers of Razers, Alienwares, and other high-performance laptops.  Maybe some of them, compelled by videographic or photographic needs, will buy Apple.  Regardless, traditional laptops and desktops represent a declining, but not moribund, method of using computers.

[Mobile is here to stay][0], but so too are traditional computers.

[0]: /2014/01/mobile-is-here-to-stay
